## LSTM based Encoder-Decoder with Multi-Head Self Attention configuration file

# Dataset & HW

use_gpu: 1
device_gpu: 0 # Default CUDA device
dataset_name: argoverse_motion_forecasting_dataset
dataset:
    path: data/datasets/argoverse/motion-forecasting/
    split: "train"
    batch_size: 512
    num_workers: 0
    start_from_percentage: 0.0 # By default, 0. If you want to analize a particular bunch of files (e.g. from 25 % to 50 %)
                               # then write here 0.25 and in split_percentage 0.25
    split_percentage: 1.0 # % of the dataset (0-1)
    shuffle: False # False to check the input data, always set to True (Only for training!)
    data_augmentation: True # Rotation, Swapping, Dropout, Gaussian noise
    class_balance: 0.5 # % of straight trajectories (considering the AGENT). Remaining % are curved trajectories
                        # (again, considering the AGENT). -1.0 if no class balance is used (get_item takes the corresponding
                        # sequence regardless if it is straight or curved)
    preprocess_data: False
    save_data: False

# Model hyperparameters

optim_parameters:
    g_learning_rate: 1.0e-3
    g_weight_decay: 0

    d_learning_rate: 1.0e-3
    d_weight_decay: 0
hyperparameters:
    num_modes: 6 # Multimodality
    obs_origin: 20 # This frame will be the origin, tipically the first observation (1) or last observation 
                   # (obs_len) of the AGENT (object to be predicted in Argoverse 1.0). Note that in the code
                   # it will be 0 and 19 respectively
    obs_len: &obs_len 20 
    pred_len: &pred_len 30 # Must be 0 for the split test since we do not have the predictions of the agents
                           # Only the observations (0 to obs_len-1 past observations)
    distance_threshold: 40 # It depends on a statistical study (see get_sequences_as_array function), 
                           # where we determine which is the optimal distance for which most agents
                           # are observed for our AGENT
    output_single_agent: True

    num_epochs: 100
    print_every: 50
    checkpoint_name: "0"                      
    checkpoint_val_percentage: 0.3 # 0.1 # (0 - 1) Check accuracy in validation split every
                                    # checkpoint_val_percentage * total_num_iterations
    checkpoint_train_percentage: 0.3 # 0.2 # (0 - 1)
    num_samples_check: 5000 # Check a maximum of num_samples_check in the check_accuracy function

    loss_type_g: "mse_w+nll" # (mse|mse_w), nll, (mse|mse_w)+nll, (mse|mse_w)+fa 
    loss_ade_weight: 1 
    loss_fde_weight: 3
    loss_nll_weight: 1.25
    loss_fa_weight: 1

    lr_scheduler: True
    lr_decay_factor: 0.65
    lr_epoch_percentage_patience: 0.1 # (0-N) E.g. If 0.05 -> the patience will be the 5 % of the total epochs
    
    g_steps: 1
    clipping_threshold_g: 1.1

    d_steps: 2
    clipping_threshold_d: 0
    
    save_root_dir: "save/argoverse"
    exp_name: # If no specific exp_name is used, fill with current day and our
    output_dir: # To be filled in the code (save_root_dir/model_name/split_percentage/exp)
    checkpoint_start_from:
    exp_description: "GAN with Old encoder/decoder"
    tensorboard_active: True

# Model

model:
    name: "gan_social_lstm_mhsa"
    adversarial_training: True
    generator:
        use_rel_disp: &use_rel_disp True # If True, use dx/dy (equivalent to velocities) 
                                          # to encode/decode. 
                                          # Otherwise, use absolute coordinates
                                          # (around target agent) and apply convolutions
                                          # to model velocity and accelerations
        use_social_attention: True

        encoder_lstm:
            noise_dim: 8
            use_rel_disp: *use_rel_disp
            conv_filters: 16
            input_len: *obs_len # Must match with num observations
            h_dim: &h_dim 256
            embedding_dim: 16 # TODO: Required?
            num_layers: 2
            bidirectional: True
            dropout: &dropout 0.5
        mhsa:
            h_dim: *h_dim
            num_heads: 4
            dropout: *dropout
        decoder_lstm:
            use_rel_disp: *use_rel_disp
            input_len: *obs_len # Must match with num observations 
            output_len: *pred_len # Must match with num predictions
            h_dim: *h_dim
            embedding_dim: 16
            num_layers: 1
            bidirectional: False
            mlp_dim: [64] # Always in list format, even if it is a single number
            dropout: *dropout
            
    discriminator:
        # Encoder
        encoder:
            num_layers: 1
            hidden_dim: 64
            emb_dim: 16 # embedding input from mlp
            mlp_config:
                dim_list: [2, 16] # From 2 (x,y) to 16 (original embedding of the paper)
                activation: 'relu'
                batch_norm: True
                dropout: 0.5
            dropout: 0

        # Classifier
        classifier:
            mlp_config:
                dim_list: [64, 1024, 1]
                activation: 'relu'
                batch_norm: True
                dropout: 0.5


