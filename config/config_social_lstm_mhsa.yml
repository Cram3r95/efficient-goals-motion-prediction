# LSTM based Encoder-Decoder with Multi-Head Self Attention configuration file

# Model hyperparameters

use_gpu: 1
device_gpu: 0 # Default CUDA device
dataset_name: argoverse_motion_forecasting_dataset
dataset:
    path: data/datasets/argoverse/motion-forecasting/
    split: "train"
    batch_size: 1024
    num_workers: 4
    start_from_percentage: 0.0 # By default, 0. If you want to analize a particular bunch of files (e.g. from 25 % to 50 %)
                               # then write here 0.25 and in split_percentage 0.25
    split_percentage: 0.01 # % of the dataset (0-1)
    shuffle: True # False to check the input data, always set to True (Only for training!)
    data_augmentation: False # Rotation, Swapping, Dropout, Gaussian noise
    class_balance: 0.7 # % of straight trajectories (considering the AGENT). Remaining % are curved trajectories
                        # (again, considering the AGENT). -1.0 if no class balance is used (get_item takes the corresponding
                        # sequence regardless if it is straight or curved)
    preprocess_data: False
    save_data: False
optim_parameters:
    g_learning_rate: 1.0e-3
    g_weight_decay: 0
hyperparameters:
    obs_origin: 20 # This frame will be the origin, tipically the first observation (1) or last observation 
                   # (obs_len) of the AGENT (object to be predicted in Argoverse 1.0). Note that in the code
                   # it will be 0 and 19 respectively
    obs_len: &obs_len 20 
    pred_len: &pred_len 30 # Must be 0 for the split test since we do not have the predictions of the agents
                           # Only the observations (0 to obs_len-1 past observations)
    distance_threshold: 40 # It depends on a statistical study (see get_sequences_as_array function), 
                           # where we determine which is the optimal distance for which most agents
                           # are observed for our AGENT
    output_single_agent: True

    num_epochs: 100
    print_every: 50
    checkpoint_name: "0"
    checkpoint_val_percentage: 0.1 # (0 - 1) Check accuracy in validation split every
                                    # checkpoint_val_percentage * total_num_iterations
    checkpoint_train_percentage: 0.2 # (0 - 1)
    num_samples_check: 5000 # Check a maximum of num_samples_check in the check_accuracy function

    loss_type_g: "mse_w+nll" # (mse|mse_w), nll, (mse|mse_w)+nll
    loss_ade_weight: 1 
    loss_fde_weight: 1.5
    loss_nll_weight: 1.25

    lr_scheduler: True
    lr_decay_factor: 0.5
    lr_epoch_percentage_patience: 0.1 # (0-N) E.g. If 0.05 -> the patience will be the 5 % of the total epochs
    
    g_steps: 1
    clipping_threshold_g: 1.1

    output_dir: "save/argoverse/social_lstm_mhsa/exp4"
    checkpoint_start_from: #"save/argoverse/social_lstm_mhsa/exp6/argoverse_motion_forecasting_dataset_0_with_model.pt"
    exp_description: "1 % dataset (both train and val).
                      Test Feasible Area Loss Function.
                      Encoder/Decoder h_dim 32 -> 256.
                      MSE (ADE&FDE) + NLL as loss function.
                      Loss with absolute and only single AGENT.
                      loss_fde_weight = 1 -> 1.5
                      Remove noise (decoder input).
                      For loop to encode trajectories.
                      Remove spatial embedding in encoder.
                      Bidirectional encoder = True.
                      num_layers encoder 1 -> 2.
                      Batch size = 1024.
                      Dropout = 0.2 -> 0.5."
    tensorboard_active: True

# Model

model:
    generator:
        encoder_lstm:
            input_len: *obs_len # Must match with num observations
            h_dim: &h_dim 256
            embedding_dim: 16 # TODO: Required?
            num_layers: 2
            bidirectional: True
            dropout: &dropout 0.5
        mhsa:
            h_dim: *h_dim
            num_heads: 4
            dropout: *dropout
        decoder_lstm:
            input_len: *obs_len # Must match with num observations 
            output_len: *pred_len # Must match with num predictions
            h_dim: *h_dim
            embedding_dim: 16
            num_layers: 1
            bidirectional: False
            mlp_dim: [64] # Always in list format, even if it is a single number
            dropout: *dropout
