Config file: ./config/config_gan_social_lstm_mhsa.yml
Configuration: 
{'use_gpu': 1, 'device_gpu': 0, 'dataset_name': 'argoverse_motion_forecasting_dataset', 'dataset': {'path': 'data/datasets/argoverse/motion-forecasting/', 'split': 'train', 'batch_size': 512, 'num_workers': 0, 'start_from_percentage': 0.0, 'split_percentage': 0.1, 'shuffle': True, 'data_augmentation': True, 'class_balance': 0.5, 'preprocess_data': False, 'save_data': False}, 'optim_parameters': {'g_learning_rate': 0.001, 'g_weight_decay': 0, 'd_learning_rate': 0.001, 'd_weight_decay': 0}, 'hyperparameters': {'num_modes': 6, 'obs_origin': 20, 'obs_len': 20, 'pred_len': 30, 'distance_threshold': 40, 'output_single_agent': True, 'num_epochs': 100, 'print_every': 50, 'checkpoint_name': '0', 'checkpoint_val_percentage': 0.3, 'checkpoint_train_percentage': 0.3, 'num_samples_check': 5000, 'loss_type_g': 'mse_w+nll', 'loss_ade_weight': 1, 'loss_fde_weight': 3, 'loss_nll_weight': 1.25, 'loss_fa_weight': 1, 'lr_scheduler': True, 'lr_decay_factor': 0.65, 'lr_epoch_percentage_patience': 0.1, 'g_steps': 1, 'clipping_threshold_g': 1.1, 'd_steps': 2, 'clipping_threshold_d': 0, 'save_root_dir': 'save/argoverse', 'exp_name': 'exp-2022-07-19_06-19', 'output_dir': 'save/argoverse/gan_social_lstm_mhsa/10.0_percent/exp-2022-07-19_06-19', 'checkpoint_start_from': None, 'exp_description': '100 % dataset (both train and val). Train with absolute coordinates, not rel-rel (only encoder). Add conv 1d to encoder (to model velocities and acceleration). Batch size = 1024. Include data augmentation (with rotation). Ensure all curved trajectories are analyzed. Encoder/Decoder h_dim 32 -> 256. MSE (ADE&FDE) + NLL as loss function. Loss with absolute and only single AGENT. loss_fde_weight = 1 -> 3 Remove noise (decoder input). For loop to encode trajectories. Remove spatial embedding in encoder. Bidirectional encoder = True. num_layers encoder 1 -> 2. Dropout = 0.2 -> 0.5.', 'tensorboard_active': True}, 'model': {'name': 'gan_social_lstm_mhsa', 'adversarial_training': True, 'generator': {'use_rel_disp': False, 'encoder_lstm': {'use_rel_disp': False, 'conv_filters': 16, 'input_len': 20, 'h_dim': 256, 'embedding_dim': 16, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.5}, 'mhsa': {'h_dim': 256, 'num_heads': 4, 'dropout': 0.5}, 'decoder_lstm': {'use_rel_disp': False, 'input_len': 20, 'output_len': 30, 'h_dim': 256, 'embedding_dim': 16, 'num_layers': 1, 'bidirectional': False, 'mlp_dim': [64], 'dropout': 0.5}}, 'discriminator': {'encoder': {'num_layers': 1, 'hidden_dim': 64, 'emb_dim': 16, 'mlp_config': {'dim_list': [2, 16], 'activation': 'relu', 'batch_norm': True, 'dropout': 0.5}, 'dropout': 0}, 'classifier': {'mlp_config': {'dim_list': [64, 1024, 1], 'activation': 'relu', 'batch_norm': True, 'dropout': 0.5}}}}, 'base_dir': '/home/robesafe/mapfe4mp'}
Initializing train dataset
Initializing val dataset
Generator model:
TrajectoryGenerator(
  (encoder): EncoderLSTM(
    (conv1): Conv1d(2, 16, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=reflect)
    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=reflect)
    (encoder): LSTM(34, 256, num_layers=2, dropout=0.5, bidirectional=True)
  )
  (lne): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (sattn): MultiHeadAttention(
    (attention): DotProductAttention(
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (W_q): Linear(in_features=256, out_features=256, bias=False)
    (W_k): Linear(in_features=256, out_features=256, bias=False)
    (W_v): Linear(in_features=256, out_features=256, bias=False)
    (W_o): Linear(in_features=256, out_features=256, bias=False)
  )
  (lnd): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp_decoder_context): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=64, out_features=256, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
  )
  (decoder): TemporalDecoderLSTM(
    (hidden2pos): Linear(in_features=256, out_features=2, bias=True)
    (spatial_embedding): Linear(in_features=40, out_features=16, bias=True)
    (ln1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
    (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (decoder): LSTM(16, 256, dropout=0.5)
  )
)
Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
Starting epoch 1
